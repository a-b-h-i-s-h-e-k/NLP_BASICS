{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44cd7af7-f45b-46e4-88fb-a12aabed3fac",
   "metadata": {},
   "source": [
    "Document Frequency(DF) = Number of times term t is present in all docs\n",
    "\n",
    "What is TF-IDF?\n",
    "\n",
    "    TF stands for Term Frequency and denotes the ratio of number of times a particular word appeared in a Document to total number of words in the document.\n",
    "\n",
    "       Term Frequency(TF) = [number of times word appeared / total no of words in a document]\n",
    "\n",
    "    Term Frequency values ranges between 0 and 1. If a word occurs more number of times, then it's value will be close to 1.\n",
    "\n",
    "    IDF stands for Inverse Document Frequency and denotes the log of ratio of total number of documents/datapoints in the whole dataset to the number of documents that contains the particular word.\n",
    "\n",
    "       Inverse Document Frequency(IDF) = [log(Total number of documents / number of documents that contains the word)]\n",
    "\n",
    "    In IDF, if a word occured in more number of documents and is common across all documents, then it's value will be less and ratio will approaches to 0.\n",
    "\n",
    "    Finally:\n",
    "\n",
    "       TF-IDF = Term Frequency(TF) * Inverse Document Frequency(IDF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3beedd0-e705-402e-b83e-0425adf0bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"Thor eating pizza, Loki is eating pizza, Ironman ate pizza already\",\n",
    "    \"Apple is announcing new iphone tomorrow\",\n",
    "    \"Tesla is announcing new model-3 tomorrow\",\n",
    "    \"Google is announcing new pixel-6 tomorrow\",\n",
    "    \"Microsoft is announcing new surface tomorrow\",\n",
    "    \"Amazon is announcing new eco-dot tomorrow\",\n",
    "    \"I am eating biryani and you are eating grapes\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bdc58fb-3010-4438-8d00-820d4260c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create the vectorizer and fit the corpus and transform them accordingly\n",
    "v = TfidfVectorizer()\n",
    "v.fit(corpus)\n",
    "transform_output = v.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b32a6c33-f487-47c5-a638-ca5e4a3af370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thor': 25, 'eating': 10, 'pizza': 22, 'loki': 17, 'is': 16, 'ironman': 15, 'ate': 7, 'already': 0, 'apple': 5, 'announcing': 4, 'new': 20, 'iphone': 14, 'tomorrow': 26, 'tesla': 24, 'model': 19, 'google': 12, 'pixel': 21, 'microsoft': 18, 'surface': 23, 'amazon': 2, 'eco': 11, 'dot': 9, 'am': 1, 'biryani': 8, 'and': 3, 'you': 27, 'are': 6, 'grapes': 13}\n"
     ]
    }
   ],
   "source": [
    "#let's print the vocabulary\n",
    "\n",
    "print(v.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47aa1c09-f5e0-4afe-9263-1105850ffa63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__sklearn_clone__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_build_request_for_signature',\n",
       " '_char_ngrams',\n",
       " '_char_wb_ngrams',\n",
       " '_check_feature_names',\n",
       " '_check_n_features',\n",
       " '_check_params',\n",
       " '_check_stop_words_consistency',\n",
       " '_check_vocabulary',\n",
       " '_count_vocab',\n",
       " '_doc_link_module',\n",
       " '_doc_link_template',\n",
       " '_doc_link_url_param_generator',\n",
       " '_get_default_requests',\n",
       " '_get_doc_link',\n",
       " '_get_metadata_request',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_limit_features',\n",
       " '_more_tags',\n",
       " '_parameter_constraints',\n",
       " '_repr_html_',\n",
       " '_repr_html_inner',\n",
       " '_repr_mimebundle_',\n",
       " '_sort_features',\n",
       " '_stop_words_id',\n",
       " '_tfidf',\n",
       " '_validate_data',\n",
       " '_validate_ngram_range',\n",
       " '_validate_params',\n",
       " '_validate_vocabulary',\n",
       " '_warn_for_unused_params',\n",
       " '_white_spaces',\n",
       " '_word_ngrams',\n",
       " 'analyzer',\n",
       " 'binary',\n",
       " 'build_analyzer',\n",
       " 'build_preprocessor',\n",
       " 'build_tokenizer',\n",
       " 'decode',\n",
       " 'decode_error',\n",
       " 'dtype',\n",
       " 'encoding',\n",
       " 'fit',\n",
       " 'fit_transform',\n",
       " 'fixed_vocabulary_',\n",
       " 'get_feature_names_out',\n",
       " 'get_metadata_routing',\n",
       " 'get_params',\n",
       " 'get_stop_words',\n",
       " 'idf_',\n",
       " 'input',\n",
       " 'inverse_transform',\n",
       " 'lowercase',\n",
       " 'max_df',\n",
       " 'max_features',\n",
       " 'min_df',\n",
       " 'ngram_range',\n",
       " 'norm',\n",
       " 'preprocessor',\n",
       " 'set_fit_request',\n",
       " 'set_params',\n",
       " 'set_transform_request',\n",
       " 'smooth_idf',\n",
       " 'stop_words',\n",
       " 'strip_accents',\n",
       " 'sublinear_tf',\n",
       " 'token_pattern',\n",
       " 'tokenizer',\n",
       " 'transform',\n",
       " 'use_idf',\n",
       " 'vocabulary',\n",
       " 'vocabulary_']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f9719da-0cff-41a7-8977-1e6c7158ab4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already : 2.386294361119891\n",
      "am : 2.386294361119891\n",
      "amazon : 2.386294361119891\n",
      "and : 2.386294361119891\n",
      "announcing : 1.2876820724517808\n",
      "apple : 2.386294361119891\n",
      "are : 2.386294361119891\n",
      "ate : 2.386294361119891\n",
      "biryani : 2.386294361119891\n",
      "dot : 2.386294361119891\n",
      "eating : 1.9808292530117262\n",
      "eco : 2.386294361119891\n",
      "google : 2.386294361119891\n",
      "grapes : 2.386294361119891\n",
      "iphone : 2.386294361119891\n",
      "ironman : 2.386294361119891\n",
      "is : 1.1335313926245225\n",
      "loki : 2.386294361119891\n",
      "microsoft : 2.386294361119891\n",
      "model : 2.386294361119891\n",
      "new : 1.2876820724517808\n",
      "pixel : 2.386294361119891\n",
      "pizza : 2.386294361119891\n",
      "surface : 2.386294361119891\n",
      "tesla : 2.386294361119891\n",
      "thor : 2.386294361119891\n",
      "tomorrow : 1.2876820724517808\n",
      "you : 2.386294361119891\n"
     ]
    }
   ],
   "source": [
    "#let's print the idf of each word\n",
    "\n",
    "all_feature_names = v.get_feature_names_out()\n",
    "\n",
    "for word in all_feature_names:\n",
    "\n",
    "    #let's get the index in the vocabulary\n",
    "    indx = v.vocabulary_.get(word)\n",
    "\n",
    "    #get the score\n",
    "    idf_score = v.idf_[indx]\n",
    "\n",
    "    print(f\"{word} : {idf_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e72b7746-1e9a-4b62-8ab4-93e0d2a17568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24266547 0.         0.         0.         0.         0.\n",
      "  0.         0.24266547 0.         0.         0.40286636 0.\n",
      "  0.         0.         0.         0.24266547 0.11527033 0.24266547\n",
      "  0.         0.         0.         0.         0.72799642 0.\n",
      "  0.         0.24266547 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.30652086 0.5680354\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.5680354  0.         0.26982671 0.\n",
      "  0.         0.         0.30652086 0.         0.         0.\n",
      "  0.         0.         0.30652086 0.        ]\n",
      " [0.         0.         0.         0.         0.30652086 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.26982671 0.\n",
      "  0.         0.5680354  0.30652086 0.         0.         0.\n",
      "  0.5680354  0.         0.30652086 0.        ]\n",
      " [0.         0.         0.         0.         0.30652086 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.5680354  0.         0.         0.         0.26982671 0.\n",
      "  0.         0.         0.30652086 0.5680354  0.         0.\n",
      "  0.         0.         0.30652086 0.        ]\n",
      " [0.         0.         0.         0.         0.30652086 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.26982671 0.\n",
      "  0.5680354  0.         0.30652086 0.         0.         0.5680354\n",
      "  0.         0.         0.30652086 0.        ]\n",
      " [0.         0.         0.49391316 0.         0.26652333 0.\n",
      "  0.         0.         0.         0.49391316 0.         0.49391316\n",
      "  0.         0.         0.         0.         0.23461736 0.\n",
      "  0.         0.         0.26652333 0.         0.         0.\n",
      "  0.         0.         0.26652333 0.        ]\n",
      " [0.         0.33794257 0.         0.33794257 0.         0.\n",
      "  0.33794257 0.         0.33794257 0.         0.56104271 0.\n",
      "  0.         0.33794257 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.33794257]]\n"
     ]
    }
   ],
   "source": [
    "#let's print the transformed output from tf-idf\n",
    "print(transform_output.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98cc8c4e-ca9d-47d1-9cb8-b728920ca98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thor eating pizza, Loki is eating pizza, Ironman ate pizza already',\n",
       " 'Apple is announcing new iphone tomorrow']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "182993de-10b0-4030-9319-87f0b5258ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24266547, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.24266547, 0.        , 0.        ,\n",
       "        0.40286636, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.24266547, 0.11527033, 0.24266547, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.72799642, 0.        , 0.        ,\n",
       "        0.24266547, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.30652086,\n",
       "        0.5680354 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.5680354 ,\n",
       "        0.        , 0.26982671, 0.        , 0.        , 0.        ,\n",
       "        0.30652086, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.30652086, 0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_output.toarray()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29cd13d-a798-40ed-babb-8e5160c4c159",
   "metadata": {},
   "source": [
    "\n",
    "Problem Statement: Given a description about a product sold on e-commerce website, classify it in one of the 4 categories\n",
    "\n",
    "Dataset Credits: https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification\n",
    "\n",
    "    This data consists of two columns.\n",
    "\n",
    "Text \tLabel\n",
    "Indira Designer Women's Art Mysore Silk Saree With Blouse Piece (Star-Red) This Saree Is Of Art Mysore Silk & Comes With Blouse Piece. \tClothing & Accessories\n",
    "IO Crest SY-PCI40010 PCI RAID Host Controller Card Brings new life to any old desktop PC. Connects up to 4 SATA II high speed SATA hard disk drives. Supports Windows 8 and Server 2012 \tElectronics\n",
    "Operating Systems in Depth About the Author Professor Doeppner is an associate professor of computer science at Brown University. His research interests include mobile computing in education, mobile and ubiquitous computing, operating systems and distribution systems, parallel computing, and security. \tBooks\n",
    "\n",
    "    Text: Description of an item sold on e-commerce website\n",
    "    Label: Category of that item. Total 4 categories: \"Electronics\", \"Household\", \"Books\" and \"Clothing & Accessories\", which almost cover 80% of any E-commerce website.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adaa3b9e-9ca7-40be-96f7-35aa8bec0e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#read the data into a pandas dataframe\n",
    "df = pd.read_csv(\"Ecommerce_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "489ef5b6-97d5-4922-8da8-57204a7da088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "076d5a17-60bf-44d9-9397-eaaf367f82f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Urban Ladder Eisner Low Back Study-Office Comp...</td>\n",
       "      <td>Household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contrast living Wooden Decorative Box,Painted ...</td>\n",
       "      <td>Household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IO Crest SY-PCI40010 PCI RAID Host Controller ...</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISAKAA Baby Socks from Just Born to 8 Years- P...</td>\n",
       "      <td>Clothing &amp; Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indira Designer Women's Art Mysore Silk Saree ...</td>\n",
       "      <td>Clothing &amp; Accessories</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                   label\n",
       "0  Urban Ladder Eisner Low Back Study-Office Comp...               Household\n",
       "1  Contrast living Wooden Decorative Box,Painted ...               Household\n",
       "2  IO Crest SY-PCI40010 PCI RAID Host Controller ...             Electronics\n",
       "3  ISAKAA Baby Socks from Just Born to 8 Years- P...  Clothing & Accessories\n",
       "4  Indira Designer Women's Art Mysore Silk Saree ...  Clothing & Accessories"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1683186-f83c-430f-a567-9a9dee4ddf99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Household                 6000\n",
       "Electronics               6000\n",
       "Clothing & Accessories    6000\n",
       "Books                     6000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the distribution of labels \n",
    "df['label'].value_counts()  # to check the class imbalance in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19bc44b-74e2-4a21-9fdb-94a4230e836d",
   "metadata": {},
   "source": [
    "From the above, we can see that almost all the labels(classes) occured equal number of times and perfectly balanced. There is no problem of class imbalance and hence no need to apply any balancing techniques like undersampling, oversampling etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d9f4d2e-6513-47f8-ba24-bbc70158f309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the new column which gives a unique number to each of these labels \n",
    "\n",
    "df['label_num'] = df['label'].map({\n",
    "    'Household' : 0,\n",
    "    'Books' :  1,\n",
    "    'Electronics' : 2,\n",
    "    'Clothing & Accessories' : 3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4568b906-0ed5-4aef-a2ac-425c7eaf45e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Urban Ladder Eisner Low Back Study-Office Comp...</td>\n",
       "      <td>Household</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contrast living Wooden Decorative Box,Painted ...</td>\n",
       "      <td>Household</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IO Crest SY-PCI40010 PCI RAID Host Controller ...</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISAKAA Baby Socks from Just Born to 8 Years- P...</td>\n",
       "      <td>Clothing &amp; Accessories</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indira Designer Women's Art Mysore Silk Saree ...</td>\n",
       "      <td>Clothing &amp; Accessories</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                   label  \\\n",
       "0  Urban Ladder Eisner Low Back Study-Office Comp...               Household   \n",
       "1  Contrast living Wooden Decorative Box,Painted ...               Household   \n",
       "2  IO Crest SY-PCI40010 PCI RAID Host Controller ...             Electronics   \n",
       "3  ISAKAA Baby Socks from Just Born to 8 Years- P...  Clothing & Accessories   \n",
       "4  Indira Designer Women's Art Mysore Silk Saree ...  Clothing & Accessories   \n",
       "\n",
       "   label_num  \n",
       "0          0  \n",
       "1          0  \n",
       "2          2  \n",
       "3          3  \n",
       "4          3  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the results \n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b654e60-1b7c-44b2-aa32-4b03bfb268eb",
   "metadata": {},
   "source": [
    "\n",
    "Train test split\n",
    "\n",
    "    Build a model with original text (no pre processing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "485a1498-2979-443b-9ac2-45372ae5ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.Text, \n",
    "    df.label_num, \n",
    "    test_size=0.2, # 20% samples will go to test dataset\n",
    "    random_state=2022,\n",
    "    stratify=df.label_num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1cbc05e-8aaa-49ad-ab0a-90701d66f4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  (19200,)\n",
      "Shape of X_test:  (4800,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b5ba296-af08-44e8-a28b-73747e77324a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15820    IRIS Furniture Children Deluxe Spiderman Toddl...\n",
       "23224    Godox CB-09 Hard Carrying Storage Suitcase Car...\n",
       "4638     Ugreen All in 1 USB 3.0 Card Reader USB Memory...\n",
       "15245    Spread Spain Metallic Gold Bar Trolley/Kitchen...\n",
       "5378     Chromozome Men's Calf Socks (Pack of 3) (SX-3 ...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c182b33-1520-4de9-998d-719551273896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_num\n",
       "0    4800\n",
       "2    4800\n",
       "3    4800\n",
       "1    4800\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db10783d-b241-4f37-8160-3b1a7a77cbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_num\n",
       "0    1200\n",
       "2    1200\n",
       "3    1200\n",
       "1    1200\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6166522-df3b-4c0a-afdd-d626dfb79087",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Attempt 1 :\n",
    "\n",
    "   1. using sklearn pipeline module create a classification pipeline to classify the Ecommerce Data.\n",
    "\n",
    "Note:\n",
    "\n",
    "    use TF-IDF for pre-processing the text.\n",
    "\n",
    "    use KNN as the classifier\n",
    "\n",
    "    print the classification report.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "216437f6-c239-4a0a-8689-c5b554cee042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95      1200\n",
      "           1       0.97      0.95      0.96      1200\n",
      "           2       0.97      0.97      0.97      1200\n",
      "           3       0.97      0.98      0.97      1200\n",
      "\n",
      "    accuracy                           0.96      4800\n",
      "   macro avg       0.96      0.96      0.96      4800\n",
      "weighted avg       0.96      0.96      0.96      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "     ('vectorizer_tfidf',TfidfVectorizer()),    \n",
    "     ('KNN', KNeighborsClassifier())         \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51e0fac1-3601-4ff7-868c-7c1c164e5844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20706    Lal Haveli Designer Handmade Patchwork Decorat...\n",
       "19166    GOTOTOP Classical Retro Cotton & PU Leather Ne...\n",
       "15209    FabSeasons Camouflage Polyester Multi Function...\n",
       "2462     Indian Superfoods: Change the Way You Eat Revi...\n",
       "6621     Milton Marvel Insulated Steel Casseroles, Juni...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcd6e9bf-8bf4-44d7-b0c4-b9b0b2feb0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20706    0\n",
       "19166    2\n",
       "15209    3\n",
       "2462     1\n",
       "6621     3\n",
       "Name: label_num, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50e0f1f5-d118-4e16-83d6-2c9dec73460c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 3, 1, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37df7a2-56a3-47b7-9893-a708d6e79038",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Attempt 2 :\n",
    "\n",
    "    using sklearn pipeline module create a classification pipeline to classify the Ecommerce Data.\n",
    "\n",
    "Note:\n",
    "\n",
    "    use TF-IDF for pre-processing the text.\n",
    "\n",
    "    use MultinomialNB as the classifier.\n",
    "\n",
    "    print the classification report.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc9cbf0f-bcb2-419f-9188-de4d7bc22c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94      1200\n",
      "           1       0.98      0.92      0.95      1200\n",
      "           2       0.97      0.97      0.97      1200\n",
      "           3       0.97      0.99      0.98      1200\n",
      "\n",
      "    accuracy                           0.96      4800\n",
      "   macro avg       0.96      0.96      0.96      4800\n",
      "weighted avg       0.96      0.96      0.96      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "     ('vectorizer_tfidf',TfidfVectorizer()),    \n",
    "     ('Multi NB', MultinomialNB())         \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d83e5-194b-4322-a8dd-17d5736f77b4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Attempt 3 :\n",
    "\n",
    "    using sklearn pipeline module create a classification pipeline to classify the Ecommerce Data.\n",
    "\n",
    "Note:\n",
    "\n",
    "    use TF-IDF for pre-processing the text.\n",
    "\n",
    "    use Random Forest as the classifier.\n",
    "\n",
    "    print the classification report.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0670a5b-5e4d-4356-ab47-7f3fe59be796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96      1200\n",
      "           1       0.98      0.98      0.98      1200\n",
      "           2       0.98      0.96      0.97      1200\n",
      "           3       0.98      0.99      0.98      1200\n",
      "\n",
      "    accuracy                           0.97      4800\n",
      "   macro avg       0.97      0.97      0.97      4800\n",
      "weighted avg       0.97      0.97      0.97      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "     ('vectorizer_tfidf',TfidfVectorizer()),        #using the ngram_range parameter \n",
    "     ('Random Forest', RandomForestClassifier())         \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff8a02a-aff2-4e73-b5a7-fcc5dbd3a5b6",
   "metadata": {},
   "source": [
    "Use text pre-processing to remove stop words, punctuations and apply lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba5ae946-8657-447c-b1e3-7c445163d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "### utlity function for pre-processing the text\n",
    "import spacy\n",
    "\n",
    "# load english language model and create nlp object from it\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "def preprocess(text):\n",
    "    # remove stop words and lemmatize the text\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "    \n",
    "    return \" \".join(filtered_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1b5362-1b72-4f1a-8572-19aac8ea3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_txt'] = df['Text'].apply(preprocess) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48556da0-2812-4f43-99fd-c872b6d22dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b6a3f-07da-44e2-986b-9ccb11bc4ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a14ae24-3ecf-4a2a-8715-a89c163391b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.preprocessed_txt[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0cfaad-7d13-43ad-b5d4-a32d3cb30e8f",
   "metadata": {},
   "source": [
    "Build a model with pre processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2224ad13-6921-450e-8abd-4881b98f7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.preprocessed_txt, \n",
    "    df.label_num,\n",
    "    test_size=0.2, # 20% samples will go to test dataset\n",
    "    random_state=2022,\n",
    "    stratify=df.label_num\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28184a67-1aed-4888-b958-ccc73ab77309",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's check the scores with our best model till now\n",
    "\n",
    "    Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e608bec6-c915-40ab-9c15-c56deed679b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "     ('vectorizer_tfidf',TfidfVectorizer()),        #using the ngram_range parameter \n",
    "     ('Random Forest', RandomForestClassifier())         \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaf82da-fee8-4d4a-99d1-c713d12c6969",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If you compare above classification report with respect to RandomForest Model with the one from unprocessed text, you will find some improvement in the model that uses preprocessed cleaned up text. The F1 score improved in the case of preprocessed data. Hence we can conclude that for this particular problem using preprocessing (removing stop words, lemmatization) is improving the performance of the model.\n",
    "\n",
    "# Plot confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2af24bc-5977-422b-83be-1f848f71abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7d0791-cb7c-4b14-a1ef-be38cd7b8b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaec390-0796-4f7b-8c6c-8c968c0580a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c45e98-6ca2-4c95-bb37-f29208824dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd25ae-2f19-49d7-8d76-a5ba5d092b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e207da75-9bd2-49fe-b142-fe22e26c2899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb5457-8fba-4408-8ca3-082b91d9ba24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe66b34-b672-4fb6-8dea-777982930724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8479eeef-1bdc-4ccc-b98b-b6f70dd1d0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
